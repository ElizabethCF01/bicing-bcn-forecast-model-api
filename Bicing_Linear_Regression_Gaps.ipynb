{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91337233",
   "metadata": {},
   "source": "# Bicing Linear Regression — Gap-Aware, Multi-Horizon\n\nThis notebook trains linear regression models to forecast `num_bikes_available` across multiple time horizons (1, 3, 6, and 12 steps ahead) for Barcelona's Bicing bike-sharing system. It implements the same gap-aware preprocessing as the LSTM workflow, processing sliding windows in a stateless manner and comparing several linear regressors (LinearRegression, Ridge, Lasso) to select the best-performing model on the validation split. The best model (Lasso with alpha=0.001) achieves an average MAE of 0.174 bikes, significantly outperforming the carry-forward baseline (MAE=12.584)."
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorgeajimenezl/Documents/harbour-space/elizabeth-eeira/bicing-prediction/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# --- Setup & Imports ---\n",
    "import os, json, math, joblib\n",
    "from collections import defaultdict, deque\n",
    "from typing import Dict, Iterator, List, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.multioutput import MultiOutputRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#stations=546, #status=2 (+1 unk)\n",
      "HORIZONS: [1, 3, 6, 12] | SEQ_LEN: 90\n",
      "Streaming chunk size: 500000\n",
      "Training samples target: 20000\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration ---\n",
    "DATA_PATH = \"data/2025_09_Setembre_BicingNou_ESTACIONS.csv\"  # <-- Change this if you use a different CSV\n",
    "META_PATH = os.path.join(\"bicing_lstm_artifacts_stateless\", \"meta.json\")\n",
    "SAVE_DIR = \"bicing_linear_artifacts\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "CSV_KW = dict(\n",
    "    sep=\",\",\n",
    "    dtype={\n",
    "        \"station_id\": \"int64\",\n",
    "        \"num_bikes_available\": \"int64\",\n",
    "        \"num_bikes_available_types.mechanical\": \"int64\",\n",
    "        \"num_bikes_available_types.ebike\": \"int64\",\n",
    "        \"num_docks_available\": \"int64\",\n",
    "        \"last_reported\": \"int64\",\n",
    "        \"is_charging_station\": \"bool\",\n",
    "        \"status\": \"string\",\n",
    "        \"is_installed\": \"int64\",\n",
    "        \"is_renting\": \"int64\",\n",
    "        \"is_returning\": \"int64\",\n",
    "        \"last_updated\": \"int64\",\n",
    "        \"ttl\": \"int64\",\n",
    "    },\n",
    "    usecols=[\n",
    "        \"station_id\",\n",
    "        \"num_bikes_available\",\n",
    "        \"num_bikes_available_types.mechanical\",\n",
    "        \"num_bikes_available_types.ebike\",\n",
    "        \"num_docks_available\",\n",
    "        \"last_reported\",\n",
    "        \"is_charging_station\",\n",
    "        \"status\",\n",
    "        \"is_installed\",\n",
    "        \"is_renting\",\n",
    "        \"is_returning\",\n",
    "        \"last_updated\",\n",
    "        \"ttl\",\n",
    "    ],\n",
    "    engine=\"c\",\n",
    ")\n",
    "\n",
    "assert os.path.exists(DATA_PATH), f\"Missing data file: {DATA_PATH}\"\n",
    "assert os.path.exists(META_PATH), f\"Run the LSTM notebook first to generate: {META_PATH}\"\n",
    "\n",
    "with open(META_PATH, \"r\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "HORIZONS = meta[\"horizons\"]\n",
    "SEQ_LEN = meta[\"seq_len\"]\n",
    "VAL_CUTOFF = meta[\"val_cutoff\"]\n",
    "BASE_STEP_SEC = meta[\"base_step_sec\"]\n",
    "SHORT_GAP_STEPS = meta[\"short_gap_steps\"]\n",
    "LONG_GAP_STEPS = meta[\"long_gap_steps\"]\n",
    "MAX_DELTA = meta[\"max_delta\"]\n",
    "BACKFILL_SHORT_GAPS = meta[\"backfill_short_gaps\"]\n",
    "\n",
    "station_list = meta[\"station_ids\"]\n",
    "status_list = meta[\"status_values\"]\n",
    "station2id = {sid: i for i, sid in enumerate(station_list)}\n",
    "status2id = {s: i for i, s in enumerate(status_list)}\n",
    "\n",
    "SCALE_MEAN = meta[\"scaler_mean\"]\n",
    "SCALE_STD = meta[\"scaler_std\"]\n",
    "\n",
    "FEAT_CONT = [\n",
    "    \"num_bikes_available\",\n",
    "    \"num_bikes_available_types.mechanical\",\n",
    "    \"num_bikes_available_types.ebike\",\n",
    "    \"num_docks_available\",\n",
    "    \"is_installed\",\n",
    "    \"is_renting\",\n",
    "    \"is_returning\",\n",
    "    \"ttl\",\n",
    "    \"sin_hour\", \"cos_hour\", \"sin_dow\", \"cos_dow\",\n",
    "    \"delta_steps\", \"log1p_delta_steps\", \"is_gap\",\n",
    "]\n",
    "TARGET_COL = \"num_bikes_available\"\n",
    "\n",
    "CHUNK_SIZE = 500_000\n",
    "TRAIN_MAX_SAMPLES = 20_000\n",
    "VAL_MAX_SAMPLES = 5_000\n",
    "TRAIN_SUBSAMPLE_PROB = 0.20\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "print(f\"#stations={len(station2id)}, #status={len(status2id)} (+1 unk)\")\n",
    "print(\"HORIZONS:\", HORIZONS, \"| SEQ_LEN:\", SEQ_LEN)\n",
    "print(\"Streaming chunk size:\", CHUNK_SIZE)\n",
    "print(\"Training samples target:\", TRAIN_MAX_SAMPLES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Stateless stream helper (linear-friendly) ---\n",
    "def add_time_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ts = pd.to_datetime(df[\"last_reported\"], unit=\"s\", utc=True)\n",
    "    ts_local = ts.dt.tz_convert(\"Europe/Madrid\")\n",
    "    df = df.copy()\n",
    "    df[\"hour\"] = ts_local.dt.hour.astype(np.int16)\n",
    "    df[\"dow\"] = ts_local.dt.dayofweek.astype(np.int16)\n",
    "    df[\"sin_hour\"] = np.sin(2 * np.pi * df[\"hour\"] / 24.0)\n",
    "    df[\"cos_hour\"] = np.cos(2 * np.pi * df[\"hour\"] / 24.0)\n",
    "    df[\"sin_dow\"] = np.sin(2 * np.pi * df[\"dow\"] / 7.0)\n",
    "    df[\"cos_dow\"] = np.cos(2 * np.pi * df[\"dow\"] / 7.0)\n",
    "    return df\n",
    "\n",
    "class StatelessSequenceStream:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        csv_kw: Dict,\n",
    "        station2id: Dict[int, int],\n",
    "        status2id: Dict[str, int],\n",
    "        scaler_mean: Dict[str, float],\n",
    "        scaler_std: Dict[str, float],\n",
    "        feat_cont: List[str],\n",
    "        target_col: str,\n",
    "        seq_len: int,\n",
    "        horizons: List[int],\n",
    "        chunk_size: int,\n",
    "        split: str,\n",
    "        val_cutoff: int,\n",
    "        base_step_sec: int,\n",
    "        short_gap_steps: int,\n",
    "        long_gap_steps: int,\n",
    "        max_delta: int,\n",
    "        backfill_short_gaps: bool,\n",
    "    ) -> None:\n",
    "        assert split in {\"train\", \"val\"}\n",
    "        self.data_path = data_path\n",
    "        self.csv_kw = csv_kw\n",
    "        self.station2id = station2id\n",
    "        self.status2id = status2id\n",
    "        self.mean = scaler_mean\n",
    "        self.std = scaler_std\n",
    "        self.feat_cont = feat_cont\n",
    "        self.target_col = target_col\n",
    "        self.seq_len = seq_len\n",
    "        self.horizons = sorted(horizons)\n",
    "        self.max_h = max(self.horizons)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.split = split\n",
    "        self.val_cutoff = val_cutoff\n",
    "        self.base_step_sec = base_step_sec\n",
    "        self.short_gap_steps = short_gap_steps\n",
    "        self.long_gap_steps = long_gap_steps\n",
    "        self.max_delta = max_delta\n",
    "        self.backfill_short_gaps = backfill_short_gaps\n",
    "        self.delta_idx = [\n",
    "            self.feat_cont.index(\"delta_steps\"),\n",
    "            self.feat_cont.index(\"log1p_delta_steps\"),\n",
    "            self.feat_cont.index(\"is_gap\"),\n",
    "        ]\n",
    "        self.buffers = defaultdict(\n",
    "            lambda: {\n",
    "                \"cont\": deque(maxlen=self.seq_len + self.max_h + 5),\n",
    "                \"status\": deque(maxlen=self.seq_len + self.max_h + 5),\n",
    "                \"target\": deque(maxlen=self.seq_len + self.max_h + 5),\n",
    "                \"time\": deque(maxlen=self.seq_len + self.max_h + 5),\n",
    "            }\n",
    "        )\n",
    "        self.last_ts_global: Dict[int, int] = {}\n",
    "\n",
    "    def _status_encode(self, series: pd.Series) -> np.ndarray:\n",
    "        unk = len(self.status2id)\n",
    "        return (\n",
    "            series.fillna(\"\")\n",
    "            .apply(lambda x: self.status2id.get(str(x), unk))\n",
    "            .astype(np.int32)\n",
    "            .to_numpy()\n",
    "        )\n",
    "\n",
    "    def _standardize_inplace(self, df: pd.DataFrame) -> None:\n",
    "        for f in self.feat_cont:\n",
    "            mu = self.mean[f]\n",
    "            sigma = self.std[f] + 1e-6\n",
    "            df[f] = (df[f].astype(\"float64\") - mu) / sigma\n",
    "\n",
    "    def __iter__(self) -> Iterator[Tuple[int, np.ndarray, np.ndarray, np.ndarray]]:\n",
    "        for chunk in pd.read_csv(self.data_path, chunksize=self.chunk_size, **self.csv_kw):\n",
    "            chunk = chunk.sort_values([\"station_id\", \"last_reported\"])\n",
    "            if self.split == \"train\":\n",
    "                chunk = chunk[chunk[\"last_reported\"] < self.val_cutoff]\n",
    "            else:\n",
    "                chunk = chunk[chunk[\"last_reported\"] >= self.val_cutoff]\n",
    "            if chunk.empty:\n",
    "                continue\n",
    "\n",
    "            chunk = add_time_features(chunk)\n",
    "\n",
    "            deltas, logs, gaps = [], [], []\n",
    "            for stn, t in zip(\n",
    "                chunk[\"station_id\"].to_numpy(),\n",
    "                chunk[\"last_reported\"].to_numpy(),\n",
    "            ):\n",
    "                prev = self.last_ts_global.get(int(stn))\n",
    "                ds = 0 if prev is None else max(0, int(round((int(t) - int(prev)) / self.base_step_sec)))\n",
    "                self.last_ts_global[int(stn)] = int(t)\n",
    "                ds = min(ds, self.max_delta)\n",
    "                deltas.append(float(ds))\n",
    "                logs.append(float(math.log1p(ds)))\n",
    "                gaps.append(float(1.0 if ds > 1 else 0.0))\n",
    "            chunk[\"delta_steps\"] = np.array(deltas, dtype=np.float32)\n",
    "            chunk[\"log1p_delta_steps\"] = np.array(logs, dtype=np.float32)\n",
    "            chunk[\"is_gap\"] = np.array(gaps, dtype=np.float32)\n",
    "\n",
    "            self._standardize_inplace(chunk)\n",
    "\n",
    "            stn_series = chunk[\"station_id\"].astype(\"int64\")\n",
    "            stn_map = stn_series.map(self.station2id)\n",
    "            valid_mask = stn_map.notna()\n",
    "            if not valid_mask.all():\n",
    "                chunk = chunk.loc[valid_mask].copy()\n",
    "                stn_map = stn_map.loc[valid_mask]\n",
    "            if chunk.empty:\n",
    "                continue\n",
    "\n",
    "            status_ids = self._status_encode(chunk[\"status\"])\n",
    "            cont_arr = chunk[self.feat_cont].astype(np.float32).to_numpy()\n",
    "            tgt_arr = chunk[self.target_col].astype(np.float32).to_numpy()\n",
    "            time_arr = chunk[\"last_reported\"].astype(np.int64).to_numpy()\n",
    "\n",
    "            for stn_idx, status_id, cont, tgt, t in zip(\n",
    "                stn_map.to_numpy(dtype=np.int32),\n",
    "                status_ids,\n",
    "                cont_arr,\n",
    "                tgt_arr,\n",
    "                time_arr,\n",
    "            ):\n",
    "                buf = self.buffers[int(stn_idx)]\n",
    "                prev_t = buf[\"time\"][-1] if buf[\"time\"] else None\n",
    "                ds = 0 if prev_t is None else max(0, int(round((int(t) - int(prev_t)) / self.base_step_sec)))\n",
    "\n",
    "                if ds > self.long_gap_steps:\n",
    "                    buf[\"cont\"].clear()\n",
    "                    buf[\"status\"].clear()\n",
    "                    buf[\"target\"].clear()\n",
    "                    buf[\"time\"].clear()\n",
    "\n",
    "                if self.backfill_short_gaps and 1 < ds <= self.short_gap_steps and buf[\"cont\"]:\n",
    "                    last_cont = buf[\"cont\"][-1].copy()\n",
    "                    last_status = buf[\"status\"][-1]\n",
    "                    last_target = buf[\"target\"][-1]\n",
    "                    last_time = buf[\"time\"][-1]\n",
    "                    for step in range(1, ds):\n",
    "                        cf = last_cont.copy()\n",
    "                        cf[self.delta_idx[0]] = 1.0\n",
    "                        cf[self.delta_idx[1]] = math.log1p(1.0)\n",
    "                        cf[self.delta_idx[2]] = 1.0\n",
    "                        buf[\"cont\"].append(cf)\n",
    "                        buf[\"status\"].append(last_status)\n",
    "                        buf[\"target\"].append(last_target)\n",
    "                        buf[\"time\"].append(last_time + step * self.base_step_sec)\n",
    "\n",
    "                buf[\"cont\"].append(cont)\n",
    "                buf[\"status\"].append(int(status_id))\n",
    "                buf[\"target\"].append(float(tgt))\n",
    "                buf[\"time\"].append(int(t))\n",
    "\n",
    "                while len(buf[\"target\"]) >= self.seq_len + self.max_h:\n",
    "                    cont_seq = list(buf[\"cont\"])[: self.seq_len]\n",
    "                    status_seq = list(buf[\"status\"])[: self.seq_len]\n",
    "                    y = [buf[\"target\"][self.seq_len - 1 + h] for h in self.horizons]\n",
    "                    yield (\n",
    "                        int(stn_idx),\n",
    "                        np.asarray(status_seq, dtype=np.int32),\n",
    "                        np.asarray(cont_seq, dtype=np.float32),\n",
    "                        np.asarray(y, dtype=np.float32),\n",
    "                    )\n",
    "                    buf[\"cont\"].popleft()\n",
    "                    buf[\"status\"].popleft()\n",
    "                    buf[\"target\"].popleft()\n",
    "                    buf[\"time\"].popleft()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train windows: 100%|██████████| 20000/20000 [00:02<00:00, 8731.56win/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 20,000 samples for split='train'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "val windows: 100%|██████████| 5000/5000 [00:02<00:00, 1951.71win/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 5,000 samples for split='val'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Sample stateless windows for linear models ---\n",
    "def collect_split(\n",
    "    split: str,\n",
    "    max_samples: int,\n",
    "    subsample_prob: float,\n",
    "    seed: int,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    stream = StatelessSequenceStream(\n",
    "        data_path=DATA_PATH,\n",
    "        csv_kw=CSV_KW,\n",
    "        station2id=station2id,\n",
    "        status2id=status2id,\n",
    "        scaler_mean=SCALE_MEAN,\n",
    "        scaler_std=SCALE_STD,\n",
    "        feat_cont=FEAT_CONT,\n",
    "        target_col=TARGET_COL,\n",
    "        seq_len=SEQ_LEN,\n",
    "        horizons=HORIZONS,\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        split=split,\n",
    "        val_cutoff=VAL_CUTOFF,\n",
    "        base_step_sec=BASE_STEP_SEC,\n",
    "        short_gap_steps=SHORT_GAP_STEPS,\n",
    "        long_gap_steps=LONG_GAP_STEPS,\n",
    "        max_delta=MAX_DELTA,\n",
    "        backfill_short_gaps=BACKFILL_SHORT_GAPS,\n",
    "    )\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    cont_rows: List[np.ndarray] = []\n",
    "    station_rows: List[int] = []\n",
    "    status_rows: List[int] = []\n",
    "    target_rows: List[np.ndarray] = []\n",
    "    last_values: List[float] = []\n",
    "\n",
    "    progress = tqdm(total=max_samples, desc=f\"{split} windows\", unit=\"win\")\n",
    "    for stn_idx, status_seq, cont_seq, target in stream:\n",
    "        if subsample_prob < 1.0 and rng.random() > subsample_prob:\n",
    "            continue\n",
    "        cont_rows.append(cont_seq.reshape(-1))\n",
    "        station_rows.append(int(stn_idx))\n",
    "        status_rows.append(int(status_seq[-1]))\n",
    "        target_rows.append(target)\n",
    "        last_std = float(cont_seq[-1, 0])\n",
    "        last_raw = last_std * (SCALE_STD[TARGET_COL] + 1e-6) + SCALE_MEAN[TARGET_COL]\n",
    "        last_values.append(last_raw)\n",
    "        progress.update(1)\n",
    "        if progress.n >= max_samples:\n",
    "            break\n",
    "    progress.close()\n",
    "\n",
    "    if not cont_rows:\n",
    "        raise RuntimeError(f\"No samples collected for split='{split}'. Check dataset and cutoff.\")\n",
    "\n",
    "    X_cont = np.stack(cont_rows).astype(np.float32)\n",
    "    stations = np.asarray(station_rows, dtype=np.int32)\n",
    "    statuses = np.asarray(status_rows, dtype=np.int32)\n",
    "    y = np.stack(target_rows).astype(np.float32)\n",
    "    last_values = np.asarray(last_values, dtype=np.float32)\n",
    "\n",
    "    print(f\"Collected {len(X_cont):,} samples for split='{split}'.\")\n",
    "    return {\n",
    "        \"X_cont\": X_cont,\n",
    "        \"station\": stations,\n",
    "        \"status\": statuses,\n",
    "        \"y\": y,\n",
    "        \"last_value\": last_values,\n",
    "    }\n",
    "\n",
    "train_data = collect_split(\n",
    "    split=\"train\",\n",
    "    max_samples=TRAIN_MAX_SAMPLES,\n",
    "    subsample_prob=TRAIN_SUBSAMPLE_PROB,\n",
    "    seed=RANDOM_STATE,\n",
    ")\n",
    "val_data = collect_split(\n",
    "    split=\"val\",\n",
    "    max_samples=VAL_MAX_SAMPLES,\n",
    "    subsample_prob=1.0,\n",
    "    seed=RANDOM_STATE + 1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (20000, 1424)\n",
      "X_val shape: (5000, 1424)\n",
      "y_train shape: (20000, 4)\n"
     ]
    }
   ],
   "source": [
    "# --- Assemble design matrices ---\n",
    "station_encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "status_encoder = OneHotEncoder(sparse_output=False, handle_unknown=\"ignore\")\n",
    "\n",
    "station_train = train_data[\"station\"][:, None]\n",
    "status_train = train_data[\"status\"][:, None]\n",
    "station_val = val_data[\"station\"][:, None]\n",
    "status_val = val_data[\"status\"][:, None]\n",
    "\n",
    "station_ohe_train = station_encoder.fit_transform(station_train).astype(np.float32)\n",
    "status_ohe_train = status_encoder.fit_transform(status_train).astype(np.float32)\n",
    "station_ohe_val = station_encoder.transform(station_val).astype(np.float32)\n",
    "status_ohe_val = status_encoder.transform(status_val).astype(np.float32)\n",
    "\n",
    "X_train = np.hstack([train_data[\"X_cont\"], station_ohe_train, status_ohe_train]).astype(np.float32)\n",
    "X_val = np.hstack([val_data[\"X_cont\"], station_ohe_val, status_ohe_val]).astype(np.float32)\n",
    "y_train = train_data[\"y\"].astype(np.float32)\n",
    "y_val = val_data[\"y\"].astype(np.float32)\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_val shape:\", X_val.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline overall MAE: 12.584\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>horizon</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>12.581079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>12.582975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>12.583575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>12.586808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   horizon        mae\n",
       "0        1  12.581079\n",
       "1        3  12.582975\n",
       "2        6  12.583575\n",
       "3       12  12.586808"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Baseline: carry-forward last observation ---\n",
    "baseline_pred = np.repeat(val_data[\"last_value\"][:, None], len(HORIZONS), axis=1)\n",
    "baseline_mae = np.mean(np.abs(baseline_pred - y_val), axis=0)\n",
    "baseline_overall = float(baseline_mae.mean())\n",
    "\n",
    "baseline_df = pd.DataFrame({\n",
    "    \"horizon\": HORIZONS,\n",
    "    \"mae\": baseline_mae,\n",
    "})\n",
    "print(f\"Baseline overall MAE: {baseline_overall:.3f}\")\n",
    "baseline_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LinearRegression...\n",
      "Training Ridge (alpha=1.0)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jorgeajimenezl/Documents/harbour-space/elizabeth-eeira/bicing-prediction/.venv/lib/python3.13/site-packages/scipy/_lib/_util.py:1233: LinAlgWarning: Ill-conditioned matrix (rcond=5.85776e-08): result may not be accurate.\n",
      "  return f(*arrays, *other_args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Ridge (alpha=10.0)...\n",
      "Training Ridge (alpha=100.0)...\n",
      "Training Lasso (alpha=0.001)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>overall_mae</th>\n",
       "      <th>h1_mae</th>\n",
       "      <th>h3_mae</th>\n",
       "      <th>h6_mae</th>\n",
       "      <th>h12_mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lasso (alpha=0.001)</td>\n",
       "      <td>0.173926</td>\n",
       "      <td>0.060068</td>\n",
       "      <td>0.139270</td>\n",
       "      <td>0.206510</td>\n",
       "      <td>0.289854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge (alpha=100.0)</td>\n",
       "      <td>0.179012</td>\n",
       "      <td>0.068164</td>\n",
       "      <td>0.143616</td>\n",
       "      <td>0.210510</td>\n",
       "      <td>0.293759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge (alpha=10.0)</td>\n",
       "      <td>0.183317</td>\n",
       "      <td>0.067688</td>\n",
       "      <td>0.146476</td>\n",
       "      <td>0.216218</td>\n",
       "      <td>0.302888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>0.184583</td>\n",
       "      <td>0.068445</td>\n",
       "      <td>0.147239</td>\n",
       "      <td>0.217332</td>\n",
       "      <td>0.305316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ridge (alpha=1.0)</td>\n",
       "      <td>0.185395</td>\n",
       "      <td>0.068971</td>\n",
       "      <td>0.148385</td>\n",
       "      <td>0.218726</td>\n",
       "      <td>0.305500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Baseline (carry-forward)</td>\n",
       "      <td>12.583610</td>\n",
       "      <td>12.581079</td>\n",
       "      <td>12.582975</td>\n",
       "      <td>12.583575</td>\n",
       "      <td>12.586808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      model  overall_mae     h1_mae     h3_mae     h6_mae  \\\n",
       "0       Lasso (alpha=0.001)     0.173926   0.060068   0.139270   0.206510   \n",
       "1       Ridge (alpha=100.0)     0.179012   0.068164   0.143616   0.210510   \n",
       "2        Ridge (alpha=10.0)     0.183317   0.067688   0.146476   0.216218   \n",
       "3          LinearRegression     0.184583   0.068445   0.147239   0.217332   \n",
       "4         Ridge (alpha=1.0)     0.185395   0.068971   0.148385   0.218726   \n",
       "5  Baseline (carry-forward)    12.583610  12.581079  12.582975  12.583575   \n",
       "\n",
       "     h12_mae  \n",
       "0   0.289854  \n",
       "1   0.293759  \n",
       "2   0.302888  \n",
       "3   0.305316  \n",
       "4   0.305500  \n",
       "5  12.586808  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Train & evaluate linear models ---\n",
    "candidates = [\n",
    "    (\"LinearRegression\", LinearRegression()),\n",
    "    (\"Ridge (alpha=1.0)\", Ridge(alpha=1.0)),\n",
    "    (\"Ridge (alpha=10.0)\", Ridge(alpha=10.0)),\n",
    "    (\"Ridge (alpha=100.0)\", Ridge(alpha=100.0)),\n",
    "    (\"Lasso (alpha=0.001)\", MultiOutputRegressor(Lasso(alpha=0.001, max_iter=5000))),\n",
    "]\n",
    "\n",
    "results = []\n",
    "for name, estimator in candidates:\n",
    "    print(f\"Training {name}...\")\n",
    "    model = estimator.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "    if preds.ndim == 1:\n",
    "        preds = preds.reshape(-1, 1)\n",
    "    mae_by_h = np.mean(np.abs(preds - y_val), axis=0)\n",
    "    overall_mae = float(mae_by_h.mean())\n",
    "    results.append({\n",
    "        \"model\": name,\n",
    "        \"estimator\": model,\n",
    "        \"mae_by_h\": mae_by_h,\n",
    "        \"overall_mae\": overall_mae,\n",
    "    })\n",
    "\n",
    "results_table = []\n",
    "baseline_row = {\"model\": \"Baseline (carry-forward)\", \"overall_mae\": baseline_overall}\n",
    "for h, mae in zip(HORIZONS, baseline_mae):\n",
    "    baseline_row[f\"h{h}_mae\"] = mae\n",
    "results_table.append(baseline_row)\n",
    "\n",
    "for r in results:\n",
    "    row = {\"model\": r[\"model\"], \"overall_mae\": r[\"overall_mae\"]}\n",
    "    for h, mae in zip(HORIZONS, r[\"mae_by_h\"]):\n",
    "        row[f\"h{h}_mae\"] = mae\n",
    "    results_table.append(row)\n",
    "\n",
    "results_df = pd.DataFrame(results_table).sort_values(\"overall_mae\").reset_index(drop=True)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: Lasso (alpha=0.001) (overall MAE=0.174)\n",
      "Saved best model to bicing_linear_artifacts/best_linear_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# --- Select best model and persist ---\n",
    "ranked = sorted(results, key=lambda r: r[\"overall_mae\"])\n",
    "best = ranked[0]\n",
    "best_name = best[\"model\"]\n",
    "best_overall = best[\"overall_mae\"]\n",
    "print(f\"Best model: {best_name} (overall MAE={best_overall:.3f})\")\n",
    "\n",
    "artifact = {\n",
    "    \"model_name\": best_name,\n",
    "    \"model\": best[\"estimator\"],\n",
    "    \"station_encoder\": station_encoder,\n",
    "    \"status_encoder\": status_encoder,\n",
    "    \"horizons\": HORIZONS,\n",
    "    \"seq_len\": SEQ_LEN,\n",
    "    \"feat_cont\": FEAT_CONT,\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"scaler_mean\": SCALE_MEAN,\n",
    "    \"scaler_std\": SCALE_STD,\n",
    "    \"train_samples\": int(train_data[\"X_cont\"].shape[0]),\n",
    "    \"val_samples\": int(val_data[\"X_cont\"].shape[0]),\n",
    "}\n",
    "artifact_path = os.path.join(SAVE_DIR, \"best_linear_model.joblib\")\n",
    "joblib.dump(artifact, artifact_path)\n",
    "print(f\"Saved best model to {artifact_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "- Evaluate the saved model on a hold-out test period or live data.\n",
    "- Integrate the artifact into the API (see `api.py`) for online predictions.\n",
    "- Experiment with additional features (e.g., weather) or alternative regularization strengths to push MAE lower.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}